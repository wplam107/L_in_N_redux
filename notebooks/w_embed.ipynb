{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T15:40:16.138710Z",
     "start_time": "2020-06-19T15:40:16.126506Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, RobustScaler\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering, DBSCAN, MeanShift\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import f1_score, accuracy_score, precision_score, recall_score\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "from we_func import remove_unwanted, preprocess_for_we"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T15:40:17.409323Z",
     "start_time": "2020-06-19T15:40:17.390997Z"
    }
   },
   "outputs": [],
   "source": [
    "# Set articles to be removed based on headlines and section of news, create stopwords from NLTK, set glove_path\n",
    "UNWANTED_HL = [\n",
    "    'UPDATE',\n",
    "    'US STOCKS',\n",
    "    'PRESS']\n",
    "\n",
    "UNWANTED_URL = [\n",
    "    '/education/',\n",
    "    '/politics/',\n",
    "    '/diplomacy/',\n",
    "    '/letters/',\n",
    "    'health-',\n",
    "    '/money/',\n",
    "    '/transport/',\n",
    "    'investing',\n",
    "    '/society/']\n",
    "\n",
    "stop_words = stopwords.words('english')\n",
    "glove_path = 'glove.6B.50d.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T15:40:19.389473Z",
     "start_time": "2020-06-19T15:40:19.344772Z"
    }
   },
   "outputs": [],
   "source": [
    "f = open('articles.p', 'rb')\n",
    "df = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T15:40:23.142051Z",
     "start_time": "2020-06-19T15:40:20.318760Z"
    }
   },
   "outputs": [],
   "source": [
    "# Preprocess the articles with functions imported from 'clean_for_we.py'\n",
    "df = df.loc[(df['source'] != 'SCMP') & (df['date'] >= pd.Timestamp(2019, 3, 15))]\n",
    "df = remove_unwanted(df, 'headline', UNWANTED_HL)\n",
    "df = remove_unwanted(df, 'url', UNWANTED_URL)\n",
    "df.reset_index(inplace=True)\n",
    "df.drop(columns='index', inplace=True)\n",
    "df = preprocess_for_we(df, stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T15:40:23.205099Z",
     "start_time": "2020-06-19T15:40:23.144329Z"
    }
   },
   "outputs": [],
   "source": [
    "word_tokens = df['word_tokens']\n",
    "vocab = []\n",
    "\n",
    "for doc in word_tokens:\n",
    "    for word in doc:\n",
    "        vocab.append(word)\n",
    "        \n",
    "vocab = set(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T15:40:24.963443Z",
     "start_time": "2020-06-19T15:40:23.214182Z"
    }
   },
   "outputs": [],
   "source": [
    "# Create dictionary of word:vector pairs from GloVe\n",
    "glove = {}\n",
    "with open(glove_path, 'rb') as f:\n",
    "    for line in f:\n",
    "        parts = line.split()\n",
    "        word = parts[0].decode('utf-8')\n",
    "        if word in vocab:\n",
    "            vector = np.array(parts[1:], dtype=np.float32)\n",
    "            glove[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-18T23:31:27.160362Z",
     "start_time": "2020-06-18T23:31:23.646830Z"
    }
   },
   "outputs": [],
   "source": [
    "# f = open('glove.p', 'wb')\n",
    "# pickle.dump(glove, f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T15:40:37.163365Z",
     "start_time": "2020-06-19T15:40:37.158313Z"
    }
   },
   "outputs": [],
   "source": [
    "# Function to map words to vectors\n",
    "def w2v(words, dictionary):\n",
    "    vec = np.mean([ dictionary[word] for word in words if word in dictionary ], axis=0)\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding per Document Mean Classification\n",
    "- XGBoost nearly across the board better than RandomForest at classifiying news article by source\n",
    "- Both were better classifying articles based on mean word vector than on topic distribution and sentiment\n",
    "- Slight improvements with GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T15:40:39.536442Z",
     "start_time": "2020-06-19T15:40:39.309784Z"
    }
   },
   "outputs": [],
   "source": [
    "word_vecs = list(df['word_tokens'].map(lambda x: w2v(x, glove)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T15:40:41.016474Z",
     "start_time": "2020-06-19T15:40:40.991209Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data=word_vecs, columns=[ f'{i}' for i in range(50) ])\n",
    "data['source'] = df['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T15:40:43.409898Z",
     "start_time": "2020-06-19T15:40:43.398027Z"
    }
   },
   "outputs": [],
   "source": [
    "# f = open('vec_data.p', 'wb')\n",
    "# pickle.dump(data, f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T15:40:46.379273Z",
     "start_time": "2020-06-19T15:40:46.373928Z"
    }
   },
   "outputs": [],
   "source": [
    "f = open('vec_data.p', 'rb')\n",
    "data = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T15:40:58.812719Z",
     "start_time": "2020-06-19T15:40:58.807788Z"
    }
   },
   "outputs": [],
   "source": [
    "X = data.drop(columns='source')\n",
    "y = data['source']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T15:40:59.679004Z",
     "start_time": "2020-06-19T15:40:59.669282Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T20:18:24.538509Z",
     "start_time": "2020-06-17T20:18:24.413409Z"
    }
   },
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=25)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T15:41:02.821830Z",
     "start_time": "2020-06-19T15:41:02.812983Z"
    }
   },
   "outputs": [],
   "source": [
    "# Scoring function\n",
    "def get_scores(model, test_data, test_labels):\n",
    "    if len(test_data) != len(test_labels):\n",
    "        return 'Data shapes incorrect'\n",
    "\n",
    "    preds = model.predict(test_data)\n",
    "    _f1 = round(f1_score(test_labels, preds, average='macro'), 4)\n",
    "    _acc = round(accuracy_score(test_labels, preds), 4)\n",
    "    _pre = round(precision_score(test_labels, preds, average='macro'), 4)\n",
    "    _rec = round(recall_score(test_labels, preds, average='macro'), 4)\n",
    "    print('F1 Score:', _f1)\n",
    "    print('Accuracy:', _acc)\n",
    "    print('Precision:', _pre)\n",
    "    print('Recall:', _rec)\n",
    "    print('--------------')\n",
    "    print(confusion_matrix(test_labels, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T20:18:27.556133Z",
     "start_time": "2020-06-17T20:18:27.534375Z"
    }
   },
   "outputs": [],
   "source": [
    "get_scores(clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T20:18:30.615836Z",
     "start_time": "2020-06-17T20:18:30.598935Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = clf.predict(X_test)\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T20:18:32.455535Z",
     "start_time": "2020-06-17T20:18:32.254732Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier(n_estimators=25)\n",
    "xgb_clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T20:18:33.731077Z",
     "start_time": "2020-06-17T20:18:33.710504Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = xgb_clf.predict(X_test)\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T20:19:11.876894Z",
     "start_time": "2020-06-17T20:19:11.858353Z"
    }
   },
   "outputs": [],
   "source": [
    "get_scores(xgb_clf, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T20:19:33.879329Z",
     "start_time": "2020-06-17T20:19:33.872747Z"
    }
   },
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier()\n",
    "scorer = make_scorer(f1_score, average = 'weighted')\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [20, 25, 30, 35],\n",
    "    'criterion': ['gini', 'entropy'],\n",
    "#     'max_depth': [5, 6, 7, 8, 9],\n",
    "    'class_weight': ['balanced', 'balanced_subsample', None],\n",
    "    'random_state': [0]\n",
    "}\n",
    "\n",
    "gs_rf = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=params,\n",
    "    scoring=scorer,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    cv=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T20:19:41.642355Z",
     "start_time": "2020-06-17T20:19:35.281771Z"
    }
   },
   "outputs": [],
   "source": [
    "gs_rf.fit(X_train, y_train)\n",
    "rf_model = gs_rf.best_estimator_\n",
    "print(30*'-')\n",
    "print('Best params:')\n",
    "print(gs_rf.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T20:19:41.662308Z",
     "start_time": "2020-06-17T20:19:41.645209Z"
    }
   },
   "outputs": [],
   "source": [
    "preds = rf_model.predict(X_test)\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-17T20:19:41.681053Z",
     "start_time": "2020-06-17T20:19:41.665075Z"
    }
   },
   "outputs": [],
   "source": [
    "get_scores(rf_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T15:41:09.875652Z",
     "start_time": "2020-06-19T15:41:09.868842Z"
    }
   },
   "outputs": [],
   "source": [
    "xgb_clf = XGBClassifier()\n",
    "scorer = make_scorer(f1_score, average = 'weighted')\n",
    "\n",
    "params = {\n",
    "    'n_estimators': [45, 50, 55],\n",
    "    'max_depth': [6, 7, 8],\n",
    "    'min_child_weight': [2, 3, 4],\n",
    "    'random_state': [0],\n",
    "}\n",
    "\n",
    "gs_xgb = GridSearchCV(\n",
    "    estimator=xgb_clf,\n",
    "    param_grid=params,\n",
    "    scoring=scorer,\n",
    "    n_jobs=-1,\n",
    "    verbose=1,\n",
    "    cv=5,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T15:41:34.779243Z",
     "start_time": "2020-06-19T15:41:11.998281Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 27 candidates, totalling 135 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    8.0s\n",
      "[Parallel(n_jobs=-1)]: Done 135 out of 135 | elapsed:   22.5s finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------\n",
      "Best params:\n",
      "{'max_depth': 7, 'min_child_weight': 3, 'n_estimators': 50, 'random_state': 0}\n"
     ]
    }
   ],
   "source": [
    "gs_xgb.fit(X_train, y_train)\n",
    "xgb_model = gs_xgb.best_estimator_\n",
    "print(30*'-')\n",
    "print('Best params:')\n",
    "print(gs_xgb.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T15:41:34.807833Z",
     "start_time": "2020-06-19T15:41:34.784059Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                 precision    recall  f1-score   support\n",
      "\n",
      "ABC (Australia)       0.67      0.22      0.33        27\n",
      "           CCTV       0.76      0.81      0.79        27\n",
      "            CNN       0.44      0.27      0.33        15\n",
      "        Reuters       0.73      0.91      0.81        91\n",
      "\n",
      "       accuracy                           0.72       160\n",
      "      macro avg       0.65      0.55      0.57       160\n",
      "   weighted avg       0.70      0.72      0.68       160\n",
      "\n"
     ]
    }
   ],
   "source": [
    "preds = xgb_model.predict(X_test)\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T15:41:37.818664Z",
     "start_time": "2020-06-19T15:41:37.795003Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score: 0.5665\n",
      "Accuracy: 0.7188\n",
      "Precision: 0.6511\n",
      "Recall: 0.5539\n",
      "--------------\n",
      "[[ 6  4  0 17]\n",
      " [ 1 22  1  3]\n",
      " [ 0  1  4 10]\n",
      " [ 2  2  4 83]]\n"
     ]
    }
   ],
   "source": [
    "get_scores(xgb_model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T15:41:44.260235Z",
     "start_time": "2020-06-19T15:41:44.253740Z"
    }
   },
   "outputs": [],
   "source": [
    "# f = open('xgb_model.p', 'wb')\n",
    "# pickle.dump(xgb_model, f)\n",
    "# f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T15:49:11.925660Z",
     "start_time": "2020-06-19T15:49:11.879508Z"
    }
   },
   "outputs": [],
   "source": [
    "f = open('xgb_model.p', 'rb')\n",
    "xgb_model = pickle.load(f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-19T15:49:13.768137Z",
     "start_time": "2020-06-19T15:49:13.728812Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.00277619, 0.00106102, 0.00367844, 0.9924844 ]], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_model.predict_proba(pd.DataFrame(X_test.iloc[0]).T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embedding Per Sentence Mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T16:12:11.109094Z",
     "start_time": "2020-06-09T16:12:10.302169Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence_vecs = df['sentence_tokens'].map(lambda x: [ w2v(sents, glove) for sents in x if len(sents) != 0 ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T16:12:11.956455Z",
     "start_time": "2020-06-09T16:12:11.929401Z"
    }
   },
   "outputs": [],
   "source": [
    "all_sentences = []\n",
    "for sents in sentence_vecs:\n",
    "    for sent in sents:\n",
    "        all_sentences.append(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T16:12:14.272407Z",
     "start_time": "2020-06-09T16:12:12.981978Z"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(data=all_sentences, columns=[ f'{i}' for i in range(50) ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-06-09T16:44:55.262990Z",
     "start_time": "2020-06-09T16:44:55.256013Z"
    }
   },
   "outputs": [],
   "source": [
    "f = open('sent_vecs.p', 'wb')\n",
    "pickle.dump(data, f)\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "redux_projects",
   "language": "python",
   "name": "redux_projects"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
